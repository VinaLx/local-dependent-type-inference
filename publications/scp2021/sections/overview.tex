\section{Overview}

In this section, we introduce \name by going through
some interesting examples to show the expressiveness and major features of the calculus.
Then we discuss the motivation, rationale of our design, and challenges.
The formal system of \name will be
discussed in Sections \ref{sec:system} and \ref{sec:metatheory}.

\subsection{A Tour of \name}
\label{sec:examples}

The \name calculus features a form of \emph{implicit
  higher-ranked polymorphism} with the power of dependent types. Thus the main feature of \name
is the ability to implicitly infer universally quantified arguments.

\lstset
  { language=Haskell
  , basicstyle=\ttfamily
  , mathescape=true
  , columns=flexible}

\paragraph{A First Example of Implicit Polymorphism}
Like most of functional languages, \name supports (implicit) parametric polymorphism.
The first simple example is the polymorphic identity function:
% \begin{flalign*}
% &\mathrm{id} : \forall (A : \star).\, A \rightarrow A &&\\
% &\mathrm{id} = \lambda (x : A).\, x &&\\
% &\mathrm{answer} : \mathbb{N} &&\\
% &\mathrm{answer} = \mathrm{id} ~ 42 \qquad \text{-- No type argument needed at the call of $\mathrm{id}$} &&
% \end{flalign*}
\begin{lstlisting}
id : $\forall$(A:$\star$). A $\rightarrow$ A
id = $\lambda$(x:A). x
answer : Nat
answer = id 42  -- No type argument needed at the call of id
\end{lstlisting}
\noindent The polymorphic parameter \lstinline{A} is annotated with its type,
which is $\star$. The type $\star$ is the type of types (also known as
\emph{kind}). In \name, the parameters of lambda abstractions are annotated
with their types, and the \lstinline{A} in the definition refers back to the
polymorphic parameter. In the examples below, we drop the parentheses around
variables and their type annotations such as \lstinline{$\lambda$x:A. x} for conciseness.

Similar to implicit polymorphism in other languages,
the polymorphic parameters of the $\forall$ types are implicitly instantiated
during applications. Thus, in the call of the identity function (\lstinline{id 42}), we
do not need to specify the argument used for instantiation. In contrast,
in an explicitly polymorphic language (such as System F) we would need
to call \lstinline{id} with an extra argument that specifies the instantiation of \lstinline{A}:
\lstinline{id Nat 42}.
% $\mathrm{id}~\mathbb{N}~ 42$.

\paragraph{Recursion and Dependent Types}

\name is dependently typed, and universal quantifications are not limited to work
on arguments of type $\star$, but it allows arguments of other types. This is
a key difference compared to much of the work on type-inference for higher-ranked
polymorphism~\citep{dunfield2013complete,le2003ml,leijen2008hmf,vytiniotis2008fph,jones2007practical}
which has been focusing on System F-like
languages where universal quantification can only have arguments of type $\star$.
Furthermore, \name supports general recursion at both the term and the type-level.

Using these features we can encode an indexed list, a \lstinline{map} operation over it
, and we illustrate how the implicit instantiation allows us to use the \lstinline{map}
function conveniently.
However, because \name is just a core calculus there is no built-in support
yet for algebraic datatypes and pattern matching.
We expect that a source language would provide a more convenient
way to define the \lstinline{map} function using pattern matching and other useful source-level
constructs. To model algebraic datatypes and pattern matching in \name, we
use an encoding by Yang and Oliveira~\citep{yang2019pure},
which is based on the Scott Encodings~\citep{mogensen1992efficient}.
The Scott Encoding encodes datatypes with different cases via
Continuation-Passing-Style (CPS) function types. The return branches of these
function types correspond to each case of the datatypes.
Case analyses of terms are encoded via applications of the CPS functions.
Since the details of the encoding are not relevant to this paper,
here we omit the code for most definitions and show only their types.

In a dependently typed language a programmer could write the following definition
for our formulation of indexed lists:
% \newcommand{\Nat}[0]{\mathbb{N}}
% \newcommand{\Succ}[0]{\mathrm{S}}
% \newcommand{\Zero}[0]{\mathrm{Z}}
% \newcommand{\List}[0]{\mathrm{List}}
% \newcommand{\Nil}[0]{\mathrm{Nil}}
% \newcommand{\Cons}[0]{\mathrm{Cons}}
% \newcommand{\map}[0]{\mathrm{map}}
% \begin{flalign*}
%   & \mathbf{data} ~ \Nat = \Zero ~|~ \Succ~\Nat &&\\
%   & \mathbf{data} ~ \List~(a : \star)~(n : \Nat) = \Nil ~ | ~ \Cons~a~(\List~a~(\Succ~n)) &&
% \end{flalign*}
\begin{lstlisting}
  data Nat = Zero | Succ Nat
  data List (a : $\star$) (n : Nat) = Nil | Cons a (List a (Succ n))
\end{lstlisting}
\noindent In this definition, the index grows towards the tail of the list,
which is admittedly not the most useful definition.
The reason why we did not choose the more practical
example, where the index represents the length of the list, is that it requires
encodings of GADT-like datatypes~\citep{gadt1,gadt2}. Such encodings are more
complex than encodings of regular algebraic datatypes as they require explicit
equality proofs and more language-level supports for such proofs~\citep{yang2019pure}. Thus we use the simpler,
but still dependently typed example here.
Here we encode \lstinline{List} and its constructors as conventional terms. We
show the definition for \lstinline{List}, and the types for the constructors next
(implementation omitted):
% \begin{flalign*}
% &\List : \star \rightarrow \Nat \rightarrow \star &&\\
% &\List = \mu L : \star \rightarrow \Nat \rightarrow \star.\, \lambda a:\star.\, \lambda n :\Nat.\, \Pi r:\star.\, r \rightarrow (a \rightarrow L~a~(\Succ ~ n) \rightarrow r) \rightarrow r &&\\
% &\Nil : \forall a : \star.\, \forall n : \Nat.\, \List ~ a ~ n &&\\
% &\Cons : \forall a : \star.\, \forall n : \Nat.\, a \rightarrow \List ~ a ~ (\Succ ~ n) \rightarrow \List ~ a ~n &&
% \end{flalign*}
\begin{lstlisting}
  List : $\star$ $\rightarrow$ Nat $\rightarrow$ $\star$
  List = $\mu$L:($\star$ $\rightarrow$ Nat $\rightarrow$ $\star$).$\lambda$a:$\star$.$\lambda$n:Nat.$\Pi$r:$\star$.
             r                          -- Nil branch
          $\rightarrow$ (a $\rightarrow$ L a (Succ n) $\rightarrow$ r) -- Cons branch
          $\rightarrow$ r
  Nil  : $\forall$a:$\star$.$\forall$n:Nat. List a n
  Cons : $\forall$a:$\star$.$\forall$n:Nat. a $\rightarrow$ List a (Succ n) $\rightarrow$ List a n
\end{lstlisting}
\noindent
In subsequent examples we will just assume some
Haskell-style syntactic sugar for datatype definitions and constructors.
Using the definitions above, we can define a \lstinline{map} function over \lstinline{List} with the type:
% \begin{flalign*}
%   & \map : \forall a : \star.\, \forall b : \star.\, \forall n : \Nat .\, (a \rightarrow b) \rightarrow \List~a~n \rightarrow \List~b~n &&
% \end{flalign*}
\begin{lstlisting}
  map : $\forall$a:$\star$.$\forall$b:$\star$.$\forall$n:Nat. a $\rightarrow$ b $\rightarrow$ List a n $\rightarrow$ List b n
\end{lstlisting}
An example of application of \lstinline{map} is:
\begin{lstlisting}
  map Succ (Cons 1 (Cons 2 Nil))
\end{lstlisting}
% \begin{flalign*}
% &\map~\Succ~(\Cons~1~(\Cons~2~\Nil)) &&
% \end{flalign*}
\noindent which increases every natural in the list by one.
Note that since the type parameters for \lstinline{map}, \lstinline{Cons}, and \lstinline{Nil}
are all implicit, they can be all omitted
and the arguments are instantiated implicitly. Thus the \lstinline{map} function
only requires two explicit arguments, making it as convenient to use
as in most functional language implementations.

There are a few final points worth mentioning about the example.
Firstly, \lstinline{List} is an example of a dependently typed function, since it is parameterized
by a natural value. Secondly, following the design of PITS~\citep{yang2019pure},
fixpoint operators ($\mu$) in \name serve a dual purpose of defining recursive types and recursive
functions. Besides its usual use of defining term-level general recursive functions,
fixpoint operators can be used to define recursive types, as shown in the encoding of \lstinline{List}
above.
Moreover, recursion is unrestricted and there is no termination checking, much like approaches
such as Dependently Typed Haskell~\citep{dh}, and unlike various other dependently typed languages
such as Agda~\citep{2007_norell_agda} or Idris~\citep{brady2013idris}.

\paragraph{Implicit Higher-Kinded Types}

The implicit capabilities also extend to the realm of higher-kinded types~\citep{tapl}.
For example, we can define a record type \lstinline{Functor},
to mimic the type class~\citep{typeclasseswadler,typeclasseskaes} \lstinline{Functor} in Haskell:
\newcommand{\Functor}[0]{\mathrm{Functor}}
\newcommand{\MkFunctor}[0]{\mathrm{MkF}}
\newcommand{\Id}[0]{\mathrm{Id}}
\newcommand{\MkId}[0]{\mathrm{MkId}}
\newcommand{\fmap}[0]{fmap}
% \begin{flalign*}
%   &\mathbf{data}~\Functor~(F : \star \rightarrow \star) = \MkFunctor~\{\fmap : \forall a : \star.\, \forall b : \star.\, (a \rightarrow b) \rightarrow F~a \rightarrow F~b\} &&
% \end{flalign*}
\begin{lstlisting}
  data Functor (F : $\star \rightarrow \star$) =
    MkF { fmap : $\forall$a:$\star$.$\forall$b:$\star$. (a $\to$ b) $\to$ F a $\to$ F b }
\end{lstlisting}
\noindent Here $\Functor$ is a record type with a single field. $\MkFunctor$ is the data constructor,
and \lstinline{fmap} is the field accessor (which encodes the type class method \lstinline{fmap}).
The type of \lstinline{fmap} is:
\begin{lstlisting}
  fmap : $\forall$F:$\star \to \star$. Functor F $\to$ ($\forall$a:$\star$.$\forall$b:$\star$. (a $\to$ b) $\to$ F a $\to$ F b)
\end{lstlisting}
Importantly this example illustrates that universal variables can quantify over higher-kinds (i.e.
\lstinline{F : $\star \to \star$}).
We can define instances of functor in a standard way:
% \begin{flalign*}
%   & \mathbf{data}~\mathrm{Id}~a=\MkId~\{runId : a\} && \\
%   & \mathrm{idFunctor} : \Functor~\Id && \\
%   & \mathrm{idFunctor} = \MkFunctor~\{\fmap = \lambda f : a \rightarrow b.\, \lambda ~x : \Id~a.\, \MkId~(f~(runId~x))\} &&
% \end{flalign*}
\begin{lstlisting}
  data Id a = MkId { runId : a }
  idFunctor : Functor Id
  idFunctor =
    MkF { fmap = $\lambda$f:(a $\to$ b).$\lambda$x:(Id a). MkId (f (runId x)) }
\end{lstlisting}
and then use \lstinline{fmap} with three arguments:
\begin{lstlisting}
  fmap idFunctor Succ (MkId 0)
\end{lstlisting}
\noindent Note that, because our calculus has no mechanism like type classes we pass the ``instance'' explicitly.
Nonetheless, three other arguments (the \lstinline{F}, \lstinline{a}, and \lstinline{b}) are implicitly instantiated.

\paragraph{Higher-Ranked Polymorphic Subtyping}
\label{sec:higher-ranked-poly}

In calculi such as the ICC~\citep{miquel2001implicit}, a form of implicit instantiation also exists.
However, such calculi do not employ subtyping, instead, they only apply instantiation
to top-level universal quantifiers. Our next example illustrates how subtyping enables
instantiation to be applied also in nested universal quantifiers, thus enabling
more types to be related.

When programming with continuations~\citep{sussman1998scheme} one of the
functions that are typically needed is call-with-current-continuation
(\verb|callcc|). In a polymorphic language, there are several types that can be
assigned to \verb|callcc|. One of these types is a rank-3 type,
while another one is a rank-1 type.
Using polymorphic subtyping we can show that the rank-3
type is more general than the rank-1 type. Thus the following program type-checks:
% \begin{flalign*}
% & \mathrm{callcc}' : \forall a : \star.\, ((\forall b : \star.\, a \rightarrow b) \rightarrow a) \rightarrow a && \\
% & \mathrm{callcc} : \forall a : \star.\, \forall b : \star.\, ((a \rightarrow b) \rightarrow a) \rightarrow a && \\
% & \mathrm{callcc} = \mathrm{callcc}' &&
% \end{flalign*}
\begin{lstlisting}
  callcc' : $\forall$a:$\star$.(($\forall$b:$\star$. a $\to$ b) $\to$ a) $\to$ a
  callcc  : $\forall$a:$\star$.$\forall$b:$\star$. ((a $\to$ b) $\to$ a) $\to$ a
  callcc = callcc'
\end{lstlisting}
\noindent The type \lstinline{$\forall$b:$\star$. a $\to$ b} appears in a positive position
of the whole signature, and it is a more general signature than \lstinline{a $\to$ b}
for an arbitrary choice of \lstinline{b}. Our language captures this subtyping relation so that
we can assign \lstinline{callcc'} to \lstinline{callcc} (but not the other way around).
In contrast, in approaches like the ICC, the types of \lstinline{callcc} and \lstinline{callcc'}
are not compatible and the example above would be rejected.

\subsection{Key Features}
\label{sec:feature-overview}

We briefly discuss the major features of \name itself and
its formalization. More formal and technical discussions will be left to
Sections \ref{sec:system} and \ref{sec:metatheory}.

\paragraph{Polymorphic Subtyping Relation}
Figure \ref{fig:polymorphic-subtyping} shows the syntax of types, monomorphic types (or monotypes),
and the polymorphic subtyping relation in \cite{dunfield2013complete} variation of
Odersky and L\"aufer's declarative type system \citep{odersky1996putting}. Although there
are slight differences between the two versions of subtyping relations, since they essentially express
the same idea, we use DK's and Odersky and L\"aufer's polymorphic subtyping relation interchangeably
in this article.
Here the syntax includes $\forall$ types that represent polymorphic types (or polytypes),
which are universally quantified over type parameters. The definition of monotypes
is standard and includes all types without occurrences of universal quantifiers.
Context $\Gamma$ is a list of type variables that are allowed to occur free in types
$A$ and $B$ in the subtyping relation.
The polymorphic subtyping relation captures a \emph{more-general-than} relation
between types. The key rules in their subtyping relation are rules $\forallL$
and $\forallR$:

\begin{itemize}
  \item In rule $\forallL$, a polytype ($\forall x.\, A$) is considered \emph{more-general}
        than another type ($B$), when we can find an arbitrary monotype ($\tau$)
        so that the instantiation is more general than $B$.
        Importantly note that this relation does not guess arbitrary (poly)types,
        but just monotypes. In other words, the relation is \emph{predicative}~\citep{Martin-Lof-1972}.
        This restriction ensures that the relation is \emph{decidable}.

  \item In rule $\forallR$ a type ($A$) is considered more general than a polytype ($\forall x. B$)
        when it is still more general than the head of the polytype, with the type
        parameter instantiated by an abstract variable $x$.
\end{itemize}

This subtyping relation sets a scene for our work, which
generalizes this relation to a dependently typed setting.

\begin{figure}
\centering
\begin{equation*}
\begin{array}{llcl}
  \text{Types} & A, B & ~\Coloneqq~ & [[x]] \mid [[int]] \mid A \rightarrow B \mid \forall x.\, A \\
  \text{Monotypes} & \tau, \sigma & ~\Coloneqq~ & [[x]] \mid [[int]] \mid \tau \rightarrow \sigma
\end{array}
\end{equation*}
\begin{drulepar}{$\Gamma \vdash_{\text{\tiny DK}} A \le B$}{Polymorphic Subtyping}
  \inferrule*[lab=${\le}\mathrm{Var}$]
    { x \in \Gamma }
    {\Gamma \dkvdash x \le x}
  \and
  \inferrule*[lab=${\le}\mathrm{Int}$]
    { }
    {\Gamma \dkvdash [[int]] \le [[int]]}
  \and
  \inferrule*[lab=${\le}{\rightarrow}$]
    {\Gamma \dkvdash B_1 \le A_1 \\ \Gamma \dkvdash A_2 \le B_2}
    {\Gamma \dkvdash A_1 \rightarrow A_2 \le B_1 \rightarrow B_2}
  \\
  \inferrule*[lab=$\forallL$]
    {\Gamma \dkvdash \tau \\ \Gamma \dkvdash [\tau / x]\, A \le B}
    {\Gamma \dkvdash \forall x.\, A \le B}
  \and
  \inferrule*[lab=$\forallR$]
    {\Gamma ,\, x \dkvdash A \le B}
    {\Gamma \dkvdash A \le \forall x.\, B}
\end{drulepar}

\caption{The \cite{dunfield2013complete} variation of the polymorphic subtyping relation by \cite{odersky1996putting}.}
\label{fig:polymorphic-subtyping}
\end{figure}

\paragraph{Generalizing Polymorphic Subtyping}
\label{sec:polymorphic-subtyping}

The parameters of universal types can only be types in the polymorphic
subtyping relation by Odersky and L\"aufer.
In \name, we generalize the polymorphic parameters so that they can
be values or other kinds of types as well.
The first idea for a direct generalization is:

\begin{mathpar}
  \inferrule*[lab=$\forallL'$]
    {\Gamma \vdash \tau \rulehl{: A} \\ \Gamma \vdash [\tau / x]\, B \le C}
    {\Gamma \vdash \forall x \rulehl{: A}.\, B \le C}
  \and
  \inferrule*[lab=$\forallR'$]
    {\Gamma ,\, x \rulehl{: B} \vdash A \le C}
    {\Gamma \vdash A \le \forall x \rulehl{: B}.\,C}
\end{mathpar}

\noindent The parameters for universal types can have any type (and not just $\star$).
Hence, instead of requiring the monotype $\tau$ to be a well-formed type in rule
$\forallL$, in rule $\forallL'$ it is
required that $\tau$ is well-typed regarding the type of the parameter
in the universal quantifier.
Similarly, for rule $\forallR'$ the context for the subtyping rule should include typing information
for the universally quantified variable.
However, this idea introduces the issue of potential mutual dependency between
subtyping and typing judgments, so further adjustments have to be made to formalize
this idea, which is discussed later in this section, Sections
\ref{sec:type-system} and \ref{sec:adaptation}.

\paragraph{Higher-Ranked Polymorphic Subtyping}

As the \verb|callcc| example in Section \ref{sec:higher-ranked-poly} shows, the subtyping
rules based on polymorphic subtyping, combined with other subtyping rules,
are able to handle the subtyping relations that occur at not only top-level
but also at a higher-ranked level. This feature distinguishes our \name from the
Implicit Calculus of Constructions (ICC) \citep{miquel2001implicit} which also discusses
the implicit polymorphism of dependent type languages. The ICC features these two related rules
in their \emph{typing} relation:

\begin{mathpar}
  \inferrule*[lab=inst]
    {[[G |- e : forall x : A. B]] \\ [[G |- e1 : A]]}
    {[[G |- e : [e1 / x] B]]}
  \and
  \inferrule*[lab=gen]
    {[[G, x : A |- e : B]] \\ [[G |- forall x : A. B : k]]}
    {[[G |- e : forall x : A. B]]}
\end{mathpar}

\noindent Without an explicit subtyping relation, the ICC is not always able
to handle subtyping at higher-ranked positions. The approach taken by the ICC
is similar to that of the Hindley-Milner type system~\citep{hindley1969principal,damas1982principal},
which is also designed for dealing only with rank-1 polymorphism.
Hindley-Milner's declarative system also has a \textsc{GEN} rule to
convert expressions to polymorphic types, and an
\textsc{INST} rule to instantiate polymorphic parameters.
Similar to \rref{GEN,INST} shown above, both rules in HM work only
for polymorphic types at top-level positions. In Hindley-Milner
the universal quantifier can only quantify over types, whereas in the ICC
it can quantify over terms of an arbitrary type (including types themselves).
% which also features similar rules in typing.
% But Hindley-Milner is designed for dealing only with rank-1 polymorphism.
In generalizations of higher-ranked polymorphic
type-inference~\citep{dunfield2013complete,le2003ml,leijen2008hmf,vytiniotis2008fph,jones2007practical},
it has been shown that rules like $\forallL$ and $\forallR$ generalize rules like
\textsc{GEN} and \textsc{INST}. Since we aim at higher-ranked polymorphic generalization,
we follow a similar, more general, approach in \name.

\paragraph{Unified Subtyping}
The revised subtyping relation with $\forallL'$ and $\forallR'$ rules suffers from an
important complication compared to the Odersky and L\"aufer formulation: there is now
a notorious mutual dependency between typing and subtyping.
In Odersky and L\"aufer's rules, the subtyping rules
do not depend on typing. In particular
the rule $\forallL$ depends only on well-formedness ($\Gamma \vdash \tau$).
In contrast, note that rule $\forallL'$ now mentions the typing relation
in its premise ($\Gamma \vdash \tau : A$). Moreover, as usual,
the subsumption rule of
the typing relation depends on the subtyping relation as shown below.
\begin{mathpar}
  \inferrule*[lab=t-sub]
    {\Gamma \vdash e : A \\ \Gamma \vdash A \le B}
    {\Gamma \vdash e : B}
\end{mathpar}
This mutual dependency has been a significant
problem when combining subtyping and dependent types~\citep{subdep, hutchins},
and presents itself on our way to the direct generalization of polymorphic subtyping.

To tackle this issue, we adopt a technique called the
\emph{unified subtyping}~\citep{full}. Unified subtyping merges the typing relation and
subtyping relation into a single relation to avoid this mutual dependency:
\begin{mathpar}
  \Gamma \vdash e_1 \le e_2 : A
\end{mathpar}
The interpretation of this judgment is: under context $\Gamma$, $e_1$ is a subtype
of $e_2$ and they both are of type $A$. The judgments for subtyping and typing
are both special forms of unified subtyping: % with the involvement of kind $[[*]]$:
\begin{mathpar}
  \Gamma \vdash A \le B \triangleq \Gamma \vdash A \le B : [[*]]
  \and
  \Gamma \vdash e : A \triangleq \Gamma \vdash e \le e : A
\end{mathpar}
The technique simplifies the formalization of dependently typed calculi with subtyping,
and especially the proof of transitivity in the original work by \cite{full}.
After applying the technique, an ideal generalization of the polymorphic
subtyping would be:

\begin{mathpar}
  \inferrule*[lab=$\forallL''$]
    {\Gamma \vdash \tau : A \\ \Gamma \vdash [\tau / x]\, B \le C \rulehl{: [[*]]}}
    {\Gamma \vdash \forall x : A.\, B \le C \rulehl{: [[*]]}}
  \and
  \inferrule*[lab=$\forallR''$]
    {\Gamma ,\, x : B \vdash A \le C \rulehl{: [[*]]}}
    {\Gamma \vdash A \le \forall x : B.\,C \rulehl{: [[*]]}}
\end{mathpar}

\noindent The basic idea of our own formalization essentially follows a similar design,
although the actual rules in \name are slightly more sophisticated.
The details will be discussed in Section \ref{sec:type-system}.

\paragraph{``Explicit'' Implicit Instantiation}

With polymorphic subtyping the instantiation of universally quantified type
parameters is done implicitly instead of being manually applied. In non-dependent
type systems, \emph{implicit} parameters are types (i.e. terms are not involved in
implicit instantiation). For example:
\begin{mathpar}
  (\lambda x.\, x)~42 \longrightarrow 42
\end{mathpar}
\noindent Here $\lambda x.\, x$ has type $\forall A.\, A \rightarrow A$, and
instantiation implicitly discovers that $A = \mathrm{Int}$.
Notably, and in contrast with explicitly polymorphic languages like System F, implicit
instantiation is not reflected anywhere at term level.
The design that we adopt still provides implicit instantiation, but
it is more explicit regarding the binding of implicit parameters.
We adopt this design to ensure that polymorphic variables are well-scoped in
type annotations of terms. Thus we use another binder, of the form $\Lambda(x : A). e$, for terms.
Nonetheless, instantiations are still
implicit as shown in the following example:
\begin{mathpar}
  (\Lambda A : [[*]].\, \lambda x : A.\, x) ~ 42 \longrightarrow 42
\end{mathpar}
Here $\Lambda A : [[*]].\, \lambda x : A.\, x$ has type $\forall A : \star. \, A \rightarrow A$,
and the polymorphic parameter $A$ is explicitly stated in the polymorphic
term. However as the reduction shows, the instantiations are still implicit.
We purposely omitted the explicit binders for implicit parameters for all the examples
in Section \ref{sec:examples} for conciseness. Such explicit binders can
be recovered with a simple form of syntactic sugar:

\[e : \forall (x : A).\, B \triangleq \Lambda x : A.\, e : \forall (x : A).\, B\]

\paragraph{Computational Irrelevance}
\label{sec:computational-irrelevance-overview}

Implicit parameters in traditional languages with polymorphic subtyping,
the ICC~\citep{miquel2001implicit,barras2008implicit}, and \name are computationaly irrelevant.
In traditional (non-dependently) typed languages, types cannot affect computation,
thus computational irrelevance is quite natural and widely adopted.
Furthermore, computational irrelevance can benefit performance, since
irrelevant arguments can simply be erased at runtime.
In dependently typed systems, however, there can be some programs where
it is useful to have computationally relevant implicit parameters.
For example, accessing the length of a length-indexed vector in constant time:
% \begin{flalign*}
%   &\mathrm{length} : \forall n : [[int]].\, \mathrm{Vector}~n \rightarrow [[int]] &&\\
%   &\mathrm{length} = \Lambda n : [[int]].\, \lambda v : \mathrm{Vector}~n.\, n
% \end{flalign*}
\begin{lstlisting}
  length : $\forall$n:Nat. Vector n $\to$ Nat
  length = $\Lambda$n:Nat.$\lambda$v:(Vector n). n
\end{lstlisting}
\noindent Here the implicit parameter \lstinline{n} is computationally relevant as it is used as
the return value of the function which is likely to be executed at runtime.
Languages like Agda, Coq, and Idris support such programs. However,
computationally relevant implicit parameters are challenging for proofs of
type soundness. Due to such challenges (see also the discussion in
Section~\ref{subsec:semantics}),
the ICC has a restriction that parameters for implicit function types
must be computationally irrelevant. Since we adopt a similar technique for the type
soundness proof, we also have a similar restriction and thus cannot encode programs such
as the above.

\paragraph{Type-level Computation and Casts}
\name features the fixpoint operator that supports general recursion at both
type and term level. In order to avoid diverging computations at type checking,
we do not provide the conversion rule (or congruence rule) like other
dependently typed systems such as the Calculus of Constructions~\citep{coc}
to support implicit type-level reduction.
\begin{mathpar}
  \inferrule*[lab=Cong]
    {[[G |- e : A]] \\ \rulehl{A =_\beta B}}
    {[[G |- e : B]]}
\end{mathpar}

\noindent The presence of the conversion rule makes the decidability of
type checking rely on the strong normalization of type-level computation
(to determine whether two types are $\beta$-equivalent).
But the presence of general recursion denies the strong normalization property
of our language.

Instead of using a conversion rule, we adopt the call-by-name design of
\emph{Pure Iso-Type Systems} (PITS)~\citep{isotype,yang2019pure},
and provide $\castdn$ and $\castup$ operators to explicitly trigger one-step
type reductions or expansions as shown in the typing rules below.
\begin{mathpar}
  \inferrule*[lab=Castup]
    {[[G |- e : B]] \\ \rulehl{[[A --> B]]} \\ [[G |- A : k]]}
    {[[G |- castup [A] e : A]]}
  \and
  \inferrule*[lab=Castdn]
    {[[G |- e : A]] \\ \rulehl{[[A --> B]]} \\ [[G |- B : k]]}
    {[[G |- castdn e : B]]}
\end{mathpar}

\noindent Now, since reductions only perform one step per use of cast
operators, whether a term strongly normalizes or not no longer affects the
decidability of type checking.
Note that there are some other cast designs in the
literature~\citep{guru,sjoberg:msfp12, kimmel:plpv, zombie:popl15},
we adopt the PITS design here for simplicity. We believe that other cast
designs could also be adopted instead, but leave this for future work.
